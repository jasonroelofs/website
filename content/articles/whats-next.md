---
title: "What's Next?"
date: 2020-11-27T16:37:22-05:00
draft: false
---

It has been some time since I last wrote about technology and specifically about software development. I've been professionally developing software for just over fifteen years, for multiple startups, as a consultant, and currently at [Stripe](https://stripe.com) building tooling and processes for making our internal services and Stripe as a whole more reliable. I can map over my career how my skills and methodologies of developing software has improved, but there is one question throughout my career that continues to bug me: What's next?

Now, this isn't a "What's next for me?" but "What's next in software and computing as a whole?". Don't get me wrong, computing is an amazing field to be a part of. The capabilities we have today were unfathomable even 10 or 20 years ago, much less when our field started back in the 50s and 60s! We have access to a huge and increasing breadth of tools to solve problems across all fields and walks of life and in the 21st century have seen a new wave of great programming languages such as Go, Rust, Elixir, and TypeScript (feel free to replace with your preferred list!). These languages let us build faster, more scalable, safer software, faster, than we've ever been able to before.

So what's the problem? As odd as it may sound, I can't help but feel that our field has stagnated. For all of the people playing around with programming languages (including [your's truely](https://github.com/jasonroelofs/language)) we have not seen nor found the next step up, the next major improvement that lets us think, process, and go even further with computers. When we dive into the [history of programming languages](https://en.wikipedia.org/wiki/History_of_programming_languages), we can see that every major paradigm in software, from procedural and imperative languages, through object-oriented, purely functional, and logical was all fleshed out and well understood by 1980, over 40 years ago. Every language since then has combined and/or improved upon these paradigms in different ways and I've yet to see any language today that makes me think "huh, there's something new here."

And if even the legendary Alan Kay, the inventor of Object-oriented programming and the SmallTalk language, who spent 16 years researching new paradigms through the [Viewpoints Research Insitute](http://www.vpri.org/) couldn't come out with any concrete results (though the papers and the research they did do is amazing and worth familiarizing yourself with), you start to wonder if there is a next step at all. Or, maybe the next step isn't going to come from programming language research at all.

Delving deeper into our field's history, you'll eventually come across a little thing called the [Dynabook](https://en.wikipedia.org/wiki/Dynabook), invented by Alan Kay, a device intended to make computing so easy and intuitive to use that it could be treated as a fundamental educational tool and would help make computing accessible to all. While Steve Jobs and the iPhone and iPad were heavily influenced by this idea, it's easily argued that Jobs missed the entire point of the Dynabook: computability for the masses.

Millions of people today carry around a computational device in their pockets that puts even the mighty Cray supercomputers of the 90s to shame, and yet the true power of these devices is locked away. We rely on others to build the apps we use to make ourselves more productive, or to just consume media, whether that be videos, music, or games. For the vast, vast majority of people, there's no other way to use these or other devices, and even for those of us who understand software and computing, it's still significantly harder than it (in my opinion) should be to make a computer do what you want.

Well, that's all true except for one paradigm described way back in 1961 and exploded into popularity in 1979 with [VisiCalc](https://en.wikipedia.org/wiki/VisiCalc): the [Spreadsheet](https://en.wikipedia.org/wiki/Spreadsheet). Whether it's Numbers on a Mac, Microsoft Excel, or Google Spreadsheets, no tool has made computing more accessible to everyone than the spreadsheet. Its paradigm is almost completely intuitive: enter in data, compute on that data. It's so useful that even the largest companies in the world still rely on spreadsheets to keep track of and run calculations on vital information. Not only that, but the spreadsheet is so intuitive that it's the go-to for nigh anyone who wants to track something on their computer! I've talked with countless people who would say that computers make them feel dumb but yet they can and do build the most elaborate programs purely in a spreadsheet.

Granted, there are plenty of problems with spreadsheets when it comes to computability, namely being that the more elaborate a spreadsheet becomes, the less maintainble it is, given the computation rules are effectively hidden from the user. But even so, nothing in 40 years has ever come close to displacing the spreadsheet as the computational tool of choice for the masses. Why? Because of a fundamental difference in paradigms:

* Software starts at the logical layer, with data being secondary.
* Spreadsheets starts at the data layer, with logic being secondary.

Or to put it in another way, spreadsheets take what people already have, the data, and then let them play with that data with logic in real time. Software requires you to think through how you want to play with your data first before any experiments can be run.
